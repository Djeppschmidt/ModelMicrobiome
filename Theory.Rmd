---
title: "Standardized Approaches to Microbial Metabarcoding Studies"
author: "Dietrich Epp Schmidt"
date: "7/25/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## introduction

For some time there has been contentious debate over the appropriate data handling techniques for microbial sequence data. The center of the controversy is the question of whether we should or should not rarefy our samples. In this case, rarefaction means subsampling each sample to a certain threshold. Typically, the practitioner will determine an arbitrary minimum count and rarefy all samples down to this count. This technique is employed to adress the lack of uniform sampling effort that is inherent in sequence data (CITATION). It should be pointed out here that subsampling sequences to an equal number of sequences is not in fact normalizing sampling effort. McMurdie and Holmes argued in 2014 that no sequences should be omitted for the purpose of normalizing sampling effort. Their argument essentially is that to do causes us to lose the power to detect taxa that respond, and increased in false positives. Later, Weiss et al countered that their method unfairly handicapped rarefied datasets in their modeling. The center of the dispute was in how the mock community was constructed, and scored. McCmurdie and Holmes had manually increased the abundance of certain taxa in the dataset. They only counted those that they had manipulated as being true positives. But if one taxon should increase in abundance, then as a proportion all other taxa will decrease. These are true effects in the sense that the relative abundance of each taxon is actually changing within the dataset. McMurdie and Holmes, however, scored those detections as false positives and found that rarefied data had a much higher probability of detecting such false positives. Weiss et all argued that in fact these were true positives, suggesting that rarefaction actually increased the sensitivity of the differential abundance analysis. They proposed using a mock community where manipulated taxa are reciprocally balanced such that they do not affect the relative abundance of any other taxon (if one taxon is increased by 10, another is decreased by 10 for balance). This differential abundance test showing that rarefaction did sometimes improve true positive detection, depending on the context. It should be pointed out here that strictly speaking, this type of behavior (i.e. perfectly balanced differences in taxon abundance) does not occure in nature.

What is perhaps most curious about this debate is what is not addressed: the underlying reason that ecologists seek to rarefy data. The concept of effort in sampling is quite literally a representation of physical effort in much of macro-ecology. It seeks to ensure that, for example, the dimensions of area surveyed in each sampling location of a study are equal so that the results are directly comparable. In sequencing studies, effort cannot be controled a priori either physically or procedurally by the scientist because the sequencing machine acts much like a random number generator in determining how many counts (often interpreted as effort) is devoted to each sample. But counts themselves are not intrinsically a measure of effort. If population A has twice the density of organisms than population B, then if samples of both are represented by the same number of sequences, then twice the effort has been expended on sampling population B compared to population A. Effort per unit sampled is important to normalize because it provides a standardization that allows the scientist to infer the density of the organisms. Therefore, to control effort is to normalize for organismal density in the real world. This key observation is lost in the discussion as none of the parties (and none of the softwares employed) include a standard protocol for normalizing for population density across samples. This is because the major publicly available softwares in this realm (particularly DESeq2, EdgeR, and Limma) were developed to determine differential expression of genes within organisms - not the differential abundance of organisms in the environment. They were designed to work with either with RNA-seq data, and often are expansions of software originally developed for micro-array analysis. In this data context, the overall abundance of RNA in the cell is rarely, if ever, an important factor to consider. Since gene expression is highly interdependent within an organism, expression data is best understood as a distribution where autocorrelation of the sequence counts is assumed. Therefore, all of these algorithms include normalization protocol for sample-wise sequencing depth within the GLM. However, for microbiome work the density of the overall community is of paramount importance. The diversity of metabolic strategies is such that groups of microbes may be essentially independent in their growth, each either relying on or being limited by different substrates. If one group becomes more abundant while the other remains at the same abundance level, it is impossible to parse whether one grew or the other died off simply using distributional (or proportional) data. In order to accurately infer the environmental drivers of each taxon, the practicioner needs to model a population density correction into the GLM. Thus, to understand how the environment shapes any microbial community, there must be an independent measure of overall community abundance. The power of external normalization has recently been demonstrated as a XYZ et. al published a paper where they were able to achieve nearly quantitative estimation of taxon abundance using QPCR as an external measure of total bacterial density. 

From the ecological perspective, much of the modeling for expression data has been done backwards. Typically, the modeler generates a random base dataset, manipulate part of it to generate a "treatment effect," then tests if the algorithm accurately identifies the manipulation. This does not allow the modeler to test the algorithm's ability to infer the state of an original population; in essence this is just a test of the algorithm's ability to infer the state of the sample. In order to benchmark the algorithm's ability to infer the population parameters, we must make a model population, sample from the model population, and then supply the algorithm with the sample data and determine its accuracy in predicting the actual state of the population. None of the popular algorithms employed for differential abundance testing have been benchmarked in this way. It also has the advantage that it eliminates subjectivity in interpreting the results because there is a modeled population in the simulation. There can be no quibble over the proper scoring for the benchmark.

Fragments:
XYZ et al's data demonstrates that highly abundant taxa are easy to model using an external... This is because normalizing the distribution to the density will
Rarefaction is often misapplied in The advantage of rarefaction is that it allows us to normalize the sequence distributio

##Methods and Results

First we use a simplistic community model framework designed to elucidate explicitly the theoretical weaknesses of different preprossessing and statistical approaches. Then we present a flexible modeling platform that allows us to construct dynamic and nuanced community models to test the bounds of the inferential statistical software.

![Analysis Workflow](images/week3/silly-dog.png)
# Model Community 1
Construct a model population. In this case we used the fibonacci sequence as a base for taxon abundance curves. We create several different detection scenarios for how the taxa could respond to the environment. Here the only environmental data is total population for each sample.

```{r}
library(phyloseq)

fib1<-c(1,1,2,3,5,8,13,21,34,55,89,144,233,377,610) #1596 (Reference conditions)
fib2<-fib1*2 #3192 (wrt ref: all taxa increase) ;(expected false: all the same)
fib3<-c(1,1,2,3,8,5,13,21,34,55,89,144,377,233,610) # (wrt ref: 5,13 increases, 6,14 decreases); expected false: NA
fib4<-fib3*2 # (wrt ref: all taxa increase); expected false(5,13 increases, 6,14 decreases)
fib5<-c(1,1,2,3,8,8,13,21,34,55,89,233,233,377,610) # a (wrt ref: 5,12 increases) ;expected false(NA)
fib6<-fib5*2 # (wrt ref: all taxa increase); expected false(5,12 increases)
fib7<-c(1,1,2,3,5,8,8,21,34,55,89,144,233,377,377) # (wrt ref: 7,15 decreases); expected false(NA)
fib8<-fib7*2 # (wrt ref: all taxa increase) ; expected false(7,15 decreases)
fib9<-c(1,1,2,0,5,0,13,21,34,55,89,144,0,377,610) # identify zeros; expected false(NA)
fib10<-fib9*2 # (wrt ref: 4,6,13 decrease, all others increase); expected false(4,6,13 decrease)
fib11<-c(1,1,2,3,5,8,13,21,34,55,89,144,466,754,1220) # (wrt ref: only top 3 increase) ; expected false(bottom 12 decrease)
fib12<-c(2,2,4,6,10,16,26,42,68,11,178,288,233,377,610) # (wrt ref: all increase except top 3 stay same); expected false(top 3 decrease)

#now let's make a community where the majority of taxa are only in one site
#first 15 are ubiquitous
fib13<-c(5,10,100,5,10,20,1,1,1,280,124,100,790,540,420,233,377,610,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
fib14<-c(5,10,100,0,0,0,540,420,1,1,1,280,5,10,20,0,0,0,0,233,377,610,0,0,0,0,0,0,0,0,0,0,0,0,0)
fib15<-c(5,10,100,0,0,0,540,420,1,1,1,280,5,540,420,0,0,0,0,0,0,0,0,0,233,377,610,0,0,0,0,0,0,0,0)
fib16<-c(5,10,100,0,0,0,540,420,1,1,1,280,5,540,420,0,0,0,0,0,0,0,0,0,0,0,0,233,377,610,0,0,0,0,0)
fib17<-c(5,rep(1,5),0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,89,144,466,233,377,610,0,0,0,0,500,50,100,200)

comm<-data.frame(fib1,fib2,fib3,fib4,fib5,fib6,fib7,fib8,fib9,fib10,fib11,fib12)
rownames(comm)<-paste(rep("sp",15), c(1:15), sep=".")

comm2<-data.frame(fib13, fib14, fib15, fib16, fib17)
rownames(comm2)<-paste(rep("sp",35), c(1:35), sep=".")
```

Now let's sample from our defined populations, and construct our sampled phyloseq object. This function is meant to simulate the sequencing machine. For this, we employ a function that samples from our population with replacement. Replacement is important because in almost all metabarcoding protocol there is amplification that creates artificial copies that follow the distribution, but not total number. In effect, sequencing is sampling with replacement. This function models sequencing also by assigning random total sample values and (unless otherwise specified) generates a random total number of sequences per sample with a mean value of 250 sequences and a variance of 75 sequences.

```{r}

make.table<-function(comm, r){ 
  
  m<-as.data.frame(t(table(sample(rownames(comm),rnorm(1, 250, 75), replace=T, prob=comm[,1]/sum(comm[,1])))))
   m<-m[,colnames(m)!="Var1"]
   colnames(m)[colnames(m)=="Freq"]<-paste("site", 1, r, sep=".")
    m2<-as.data.frame(t(table(sample(rownames(comm),rnorm(1, 250, 75), replace=T, prob=comm[,2]/sum(comm[,2])))))
   m<-merge(m, m2, by="Var2", all=T)
    m<-m[,colnames(m)!="Var1"]
    colnames(m)[colnames(m)=="Freq"]<-paste("site", 2, r, sep=".")
    for(i in 3:ncol(comm)){
     a<-as.data.frame(t(table(sample(rownames(comm),rnorm(1, 250, 75), replace=T, prob=comm[,i]/sum(comm[,i])))))
     a<-a[,colnames(a)!="Var1"]
     m<-merge(m,a, by="Var2", all=T)
      m<-m[,colnames(m)!="Var1"]
      colnames(m)[colnames(m)=="Freq"]<-paste("site", i, r, sep=".")
      
    }
    #colnames(m)<-c("Var1", paste0("site",c(1:12), r, sep="."))
      #names(m)[names(m)=="Var2"]<-paste0("Sample", r,)
      rownames(m)<-m$Var2
      m
    }

model.rarefy<-function(comm, rep){
 a<-make.table(comm, r=1)
 #a<-a[,colnames(a)!="Var1"]
 for(r in 2:rep){
   b<-make.table(comm, r)
   a<-merge(a,b,by="Var2", all=T)
   a<-a[,colnames(a)!="Var1"]}
   rownames(a)<-a$Var2
   a<-a[,colnames(a)!="Var2"]
   a[is.na(a)] <- 0
   a
 }




model1.otu<-model.rarefy(comm, 3) # works
s.order1<-c("site.1.1", "site.1.2", "site.1.3", "site.2.1", "site.2.2", "site.2.3", "site.3.1", "site.3.2", "site.3.3", "site.4.1", "site.4.2", "site.4.3", "site.5.1","site.5.2","site.5.3", "site.6.1","site.6.2","site.6.3", "site.7.1","site.7.2","site.7.3", "site.8.1","site.8.2","site.8.3", "site.9.1","site.9.2","site.9.3", "site.10.1","site.10.2","site.10.3", "site.11.1","site.11.2","site.11.3", "site.12.1","site.12.2","site.12.3")
model1.otu<-model1.otu[,s.order1]

model2.otu<-model.rarefy(comm=comm2, 5) # works
s.order2<-c("site.1.1", "site.1.2", "site.1.3", "site.1.4", "site.1.5", "site.2.1", "site.2.2", "site.2.3", "site.2.4","site.2.5","site.3.1", "site.3.2", "site.3.3","site.3.4","site.3.5", "site.4.1", "site.4.2", "site.4.3","site.4.4","site.4.5", "site.5.1","site.5.2","site.5.3", "site.5.4","site.5.5")
model2.otu<-model2.otu[,s.order2]

# construct the metadata
meta1<-data.frame("Factor" = as.factor(c(rep("one",3), rep("two",3),rep("three",3),rep("four",3), rep("five",3), rep("six",3),rep("seven",3),rep("eight",3),rep("nine",3),rep("ten",3),rep("eleven",3),rep("twelve",3))), "Sample" = c("site.1.1", "site.1.2", "site.1.3", "site.2.1", "site.2.2", "site.2.3", "site.3.1", "site.3.2", "site.3.3", "site.4.1", "site.4.2", "site.4.3", "site.5.1","site.5.2","site.5.3", "site.6.1","site.6.2","site.6.3", "site.7.1","site.7.2","site.7.3", "site.8.1","site.8.2","site.8.3", "site.9.1","site.9.2","site.9.3", "site.10.1","site.10.2","site.10.3", "site.11.1","site.11.2","site.11.3", "site.12.1","site.12.2","site.12.3"), "Density"=c(rep(sum(fib1), 3),rep(sum(fib2), 3),rep(sum(fib3), 3),rep(sum(fib4), 3),rep(sum(fib5), 3),rep(sum(fib6), 3),rep(sum(fib7), 3),rep(sum(fib8), 3),rep(sum(fib9), 3),rep(sum(fib10), 3),rep(sum(fib11), 3),rep(sum(fib12), 3)))
rownames(meta1)<-meta$Sample
model1.ps<-phyloseq(otu_table(model1.otu, taxa_are_rows = T), sample_data(meta1))
sample_data(model1.ps)$Total_abundance<-sample_sums(model1.ps)
sample_data(model1.ps)$DensityF.model1<-sample_data(model1.ps)$Density/mean(sample_data(model1.ps)$Density)


meta2<-data.frame("Factor" = as.factor(c(rep("one",5), rep("two",5),rep("three",5),rep("four",5), rep("five",5))), "Sample" = c("site.1.1", "site.1.2", "site.1.3", "site.1.4", "site.1.5", "site.2.1", "site.2.2", "site.2.3", "site.2.4","site.2.5","site.3.1", "site.3.2", "site.3.3","site.3.4","site.3.5", "site.4.1", "site.4.2", "site.4.3","site.4.4","site.4.5", "site.5.1","site.5.2","site.5.3", "site.5.4","site.5.5"), "Density"=c(rep(sum(fib13), 5),rep(sum(fib14), 5),rep(sum(fib15), 5),rep(sum(fib16), 5),rep(sum(fib17), 5)))
rownames(meta2)<-meta2$Sample
model2.ps<-phyloseq(otu_table(model2.otu, taxa_are_rows = T), sample_data(meta2))
sample_data(model2.ps)$Total_abundance<-sample_sums(model2.ps)
sample_data(model2.ps)$DensityF.model2<-sample_data(model2.ps)$Density/mean(sample_data(model2.ps)$Density)


saveRDS(model.ps, "~/Documents/GitHub/ModelMicrobiome/ModelPS.RDS")
?phyloseq::subset_samples
```
There are several philosophies about how to normalize the sample data. Some say not to do any normalization, to leave counts as they are (method=untouched); some say that the sequence counts should be variance stabilized- there are several different methods (method=deseqVST and limmaVST); some say that the data should be rarefied to an even sampling depth (method=evenRarefied); some say that we should rarefy to depths that are proportional to an independent measure of overall population size (method=proportionalRarefied); and finally some say that we should simply scale the sample distributions to an independent measure of population size (method=scaled). We do all of these for comparison. We will simulate QPCR of our target gene using the sums of the modeled population. This will be a vector called (Density)

```{r}

make.rarefy<-function(x, level){
  require(phyloseq)
  require(vegan)
  
  if (length(level)==1){
     p<-prune_samples(sample_sums(x)>=level, x)
  
  if (nsamples(x)>nsamples(p)){warning(as.character(nsamples(x)-nsamples(p)), " samples have been removed because they are lower than rarefaction limit")}
     
  r<-as.data.frame(as.matrix(otu_table(p)))
  meta<-sample_data(p)
  rr<-rrarefy(t(r), level) #use lowest value
  ps<-phyloseq(otu_table(t(rr), taxa_are_rows = T), sample_data(meta))
  ps} else {
    sample_data(x)$adj<-level
    
     p<-prune_samples(sample_sums(x)>=level, x)
  if (nsamples(x)>nsamples(p)){warning(as.character(nsamples(x)-nsamples(p)), " samples have been removed because they are lower than rarefaction limit")}
    
    r<-as.data.frame(as.matrix(otu_table(p)))
  meta<-sample_data(p)
  rr<-rrarefy(t(r), meta$adj)
  ps<-phyloseq(otu_table(t(rr), taxa_are_rows = T), sample_data(meta))
  ps
  }
}

make.scaled<-function(ps, val, scale){
  scaled<-data.frame(mapply(`*`, data.frame(as.matrix(otu_table(transform_sample_counts(ps, function(x) x/sum(x))))), scale * val))# sample_data(ps)$val))
  names<-rownames(data.frame(as.matrix(otu_table(ps))))
  rownames(scaled)<-names
  scaled<-round(scaled)

  p2<-ps
  otu_table(p2)<- otu_table(scaled, taxa_are_rows=T)
  p2
}

# untouched ####
untouched<-model.ps

# deseqVST ####
gm_mean = function(x, na.rm=TRUE){
  exp(sum(log(x[x > 0]), na.rm=na.rm) / length(x))
} # relative abundance

make.deseqVST<-function(ps, Factor, l){
r<-phyloseq_to_deseq2(ps, ~Factor)
geoMeans = apply(counts(r), 1, gm_mean)
dds = estimateSizeFactors(r, geoMeans = geoMeans)
#dds<-DESeqDataSetFromMatrix(r, sample_data(ps), design=~Factor)
#dds = estimateSizeFactors(dds)
if (l==1){dds = estimateDispersions(dds)} else {
dds <- estimateDispersionsGeneEst(dds)
 dispersions(dds) <- mcols(dds)$dispGeneEst
  }
vst = getVarianceStabilizedData(dds)
deseqVST<-ps
otu_table(deseqVST) <- otu_table(vst, taxa_are_rows = TRUE)
deseqVST
}
# LimmaVST ###

make.limmaVST<-function(ps, Factor){
  
  counts<-as.data.frame(as.matrix(otu_table(ps)))
  factors<-unlist(sample_data(ps)[,Factor])
  design<-model.matrix(~factors)
  dge <- DGEList(counts=counts)
  dge <- calcNormFactors(dge) #what happens if we don't do this step?
  v<-voom(dge, design, plot=F)
  LimmaVST<-ps
  otu_table(LimmaVST)<-otu_table(v$E, taxa_are_rows = T)
  LimmaVST
}


```

Let's now run the functions to get all the different combinations of preprocessing...


```{r}

# evenRarefied ####
model1.eRare<-make.rarefy(model1.ps, 100)  
model2.eRare<-make.rarefy(model2.ps, 100)


# proportionalRarefied ####
model1.pRare<-make.rarefy(model1.ps, 100 * sample_data(model1.ps)$DensityF.model1)  
model2.pRare<-make.rarefy(model2.ps, 100 * sample_data(model2.ps)$DensityF.model2)  


# scaled ####
model1.scaled<-make.scaled(model1.ps, val=sample_data(model1.ps)$DensityF.model1, scale = 100)
model2.scaled<-make.scaled(model2.ps, val=sample_data(model2.ps)$DensityF.model2, scale = 100)

# limma vst ####
model1.Limma.raw<-make.limmaVST(model1.ps, "Factor")
model1.Limma.eRare<-make.limmaVST(model1.eRare, "Factor")
model1.Limma.pRare<-make.limmaVST(model1.pRare, "Factor")
model1.Limma.scaled<-make.limmaVST(model1.scaled, "Factor")

model2.Limma.raw<-make.limmaVST(model2.ps, "Factor")
model2.Limma.eRare<-make.limmaVST(model2.eRare, "Factor")
model2.Limma.pRare<-make.limmaVST(model2.pRare, "Factor")
model2.Limma.scaled<-make.limmaVST(model2.scaled, "Factor")

# Deseq2 vst ####

model1.deseq.raw<-make.deseqVST(model1.ps, "Factor", l=1)
model1.deseq.eRare<-make.deseqVST(model1.eRare, "Factor", l=2) # error not enough dispersion
model1.deseq.pRare<-make.deseqVST(model1.pRare, "Factor") # error not enough dispersion
model1.deseq.scaled<-make.deseqVST(model1.scaled, "Factor") # error not enough dispersion

model2.deseq.raw<-make.deseqVST(model2.ps, "Factor")
model2.deseq.eRare<-make.deseqVST(model2.eRare, "Factor")
model2.deseq.pRare<-make.deseqVST(model2.pRare, "Factor")
model2.deseq.scaled<-make.deseqVST(model2.scaled, "Factor")

```
Build a script to run the following platforms:

DESeq2
Limma Trend <- uses EdgeR 
EdgeR <- already there
BBSeq  <- nope! bisulfide sequencing...
DSS <- um...
BaySeq <- meh
ShrinkBayes <- meh
PoissonSeq <- this one!!

Now we build out the test key
```{r}
# zero is no change, 1 is increase, -1 is decrease
model.key<-data.frame("one"=c(rep(0,15)),"two"=c(rep(1,15)), "three" = c(0,0,0,0,1,-1,0,0,0,0,0,0,1,-1,0), "four" =c(rep(1, 15)), "five"= c(0,0,0,0,1,0,0,0,0,0,0,1,0,0,0), "six"=c(rep(1, 15)), "seven"=c(0,0,0,0,0,0,-1,0,0,0,0,0,0,0,-1), "eight"=c(rep(1,15)), "nine"=c(0,0,0,-1,0,-1,0,0,0,0,0,0,-1,0,0), "ten"=c(1,1,1,-1,1,-1,1,1,1,1,1,1,-1,1,1), "eleven"=c(0,0,0,0,0,0,0,0,0,0,0,0,1,1,1), "twelve"= c(1,1,1,1,1,1,1,1,1,1,1,1,0,0,0))
rownames(model.key)<-paste("sp", 1:15, sep=".")
model.key
```

Run test of environmental correlation:
```{r}
<-function(x, model){
  require(vegan)
  a<-adonis()
  summary(a)
  
}
```
If we want to test for all decreasing effects, we can set number two as zero in the comparison tables.


Run test of indicators:
```{r}

limma.Indics<-function(ps, Factor){
  
  counts<-as.data.frame(as.matrix(otu_table(ps)))
  factors<-sample_data(ps)$Factor
  factors<-factor(factors, levels(factors)[c(6,12,10,4,3,8,7,1,5,9,2,11)])
  design<-model.matrix(~0+factors)
  contr.matrix<- makeContrasts(
  TwoVOne = factorstwo-factorsone, 
  ThreeVOne = factorsthree-factorsone, 
  FourVOne = factorsfour-factorsone, 
  FiveVOne = factorsfive-factorsone,
  SixVOne = factorssix-factorsone,
  SevenVOne = factorsseven-factorsone,
  EightVOne = factorseight-factorsone,
  NineVOne = factorsnine-factorsone,
  TenVOne = factorsten-factorsone, 
  ElevenVOne = factorseleven-factorsone,
  TwelveVOne = factorstwelve-factorsone, 
  levels = colnames(design))
  dge <- DGEList(counts=counts)
  dge <- calcNormFactors(dge) #what happens if we don't do this step?
  v<-voom(dge, design, plot=F)
  fitV <- lmFit(v, design)
  fitV <- contrasts.fit(fitV, contrasts=contr.matrix)
  fitV <- eBayes(fitV, trend=TRUE)
  sig<-decideTests(fitV)
  sig
}

library(data.table)
model1.Raw.Limmaindics<-as.data.frame(limma.Indics(model1.ps, "Factor"))
model1.eRare.Limmaindics<-as.data.frame(limma.Indics(model1.eRare, "Factor"))
model1.pRare.Limmaindics<-as.data.frame(limma.Indics(model1.pRare, "Factor"))
model1.scaled.Limmaindics<-as.data.frame(limma.Indics(model1.scaled, "Factor"))

summary(aov(as.data.frame(as.matrix(t(otu_table(model1.eRare))))$sp.14~sample_data(model1.eRare)$Factor))
t<-TukeyHSD(aov(as.data.frame(as.matrix(t(otu_table(model1.eRare))))$sp.14~sample_data(model1.eRare)$Factor))
t2<-as.data.frame(t$`sample_data(model1.eRare)$Factor`[rownames(t$`sample_data(model1.eRare)$Factor`) %like% "one",])

summary(aov(as.data.frame(as.matrix(t(otu_table(model1.pRare))))$sp.14~sample_data(model1.pRare)$Factor))
t<-TukeyHSD(aov(as.data.frame(as.matrix(t(otu_table(model1.pRare))))$sp.14~sample_data(model1.pRare)$Factor))
prare.t2<-as.data.frame(t$`sample_data(model1.pRare)$Factor`[rownames(t$`sample_data(model1.pRare)$Factor`) %like% "one",])

summary(aov(as.data.frame(as.matrix(t(otu_table(model1.scaled))))$sp.14~sample_data(model1.scaled)$Factor))
t<-TukeyHSD(aov(as.data.frame(as.matrix(t(otu_table(model1.scaled))))$sp.14~sample_data(model1.scaled)$Factor))
scaled.t2<-as.data.frame(t$`sample_data(model1.scaled)$Factor`[rownames(t$`sample_data(model1.scaled)$Factor`) %like% "one",])

summary(aov(as.data.frame(as.matrix(t(otu_table(model1.ps))))$sp.14~sample_data(model1.ps)$Factor))
t<-TukeyHSD(aov(as.data.frame(as.matrix(t(otu_table(model1.ps))))$sp.14~sample_data(model1.ps)$Factor))
raw.t2<-as.data.frame(t$`sample_data(model1.ps)$Factor`[rownames(t$`sample_data(model1.ps)$Factor`) %like% "one",])

summary(aov(as.data.frame(as.matrix(t(otu_table(model1.Limma.raw))))$sp.14~sample_data(model1.Limma.raw)$Factor))
t<-TukeyHSD(aov(as.data.frame(as.matrix(t(otu_table(model1.Limma.raw))))$sp.14~sample_data(model1.Limma.raw)$Factor))
limmaVST.t2<-as.data.frame(t$`sample_data(model1.Limma.raw)$Factor`[rownames(t$`sample_data(model1.Limma.raw)$Factor`) %like% "one",])

summary(aov(as.data.frame(as.matrix(t(otu_table(model1.deseq.raw))))$sp.14~sample_data(model1.deseq.raw)$Factor))
t<-TukeyHSD(aov(as.data.frame(as.matrix(t(otu_table(model1.deseq.raw))))$sp.14~sample_data(model1.deseq.raw)$Factor))
t2<-as.data.frame(t$`sample_data(model1.deseq.raw)$Factor`[rownames(t$`sample_data(model1.deseq.raw)$Factor`) %like% "one",])


heatdf<-data.frame("AOV.scaled"=scaled.t2$`p adj`, "AOV.Prare"=prare.t2$`p adj`, "AOV.raw"=raw.t2$`p adj`, "AOV.limmaVST"=limmaVST.t2$`p adj`)
k1<-c(1,1,0,1,0,0,1,1,1,0,1)
k1<-c(T,T,F,T,F,F,T,T,T,F,T)
k.df<-c(k1, k1, k1, k1)
heatdf<-heatdf<=0.05

heat.df<-heatdf==k.df
heat.df$ID<-rownames(t2)

heat<-melt(heat.df, id.var=ID)
ggplot(heat, aes())


rownames(heat.df)<-rownames(t2)
heat.df<-lapply(heat.df, numeric)
heatmap(numeric(as.character(heat.df)))
```

```{R}

model2.Raw.Limmaindics<-as.data.frame(limma.Indics2(model2.ps, "Factor"))
model2.eRare.Limmaindics<-as.data.frame(limma.Indics(model2.eRare, "Factor"))
model2.pRare.Limmaindics<-as.data.frame(limma.Indics(model2.pRare, "Factor"))
model2.scaled.Limmaindics<-as.data.frame(limma.Indics(model2.scaled, "Factor"))

summary(aov(as.data.frame(as.matrix(t(otu_table(model1.eRare))))$sp.14~sample_data(model1.eRare)$Factor))
t<-TukeyHSD(aov(as.data.frame(as.matrix(t(otu_table(model1.eRare))))$sp.14~sample_data(model1.eRare)$Factor))
t2<-as.data.frame(t$`sample_data(model1.eRare)$Factor`[rownames(t$`sample_data(model1.eRare)$Factor`) %like% "one",])

summary(aov(as.data.frame(as.matrix(t(otu_table(model1.pRare))))$sp.14~sample_data(model1.pRare)$Factor))
t<-TukeyHSD(aov(as.data.frame(as.matrix(t(otu_table(model1.pRare))))$sp.14~sample_data(model1.pRare)$Factor))
prare.t2<-as.data.frame(t$`sample_data(model1.pRare)$Factor`[rownames(t$`sample_data(model1.pRare)$Factor`) %like% "one",])

summary(aov(as.data.frame(as.matrix(t(otu_table(model1.scaled))))$sp.14~sample_data(model1.scaled)$Factor))
t<-TukeyHSD(aov(as.data.frame(as.matrix(t(otu_table(model1.scaled))))$sp.14~sample_data(model1.scaled)$Factor))
scaled.t2<-as.data.frame(t$`sample_data(model1.scaled)$Factor`[rownames(t$`sample_data(model1.scaled)$Factor`) %like% "one",])

summary(aov(as.data.frame(as.matrix(t(otu_table(model1.ps))))$sp.14~sample_data(model1.ps)$Factor))
t<-TukeyHSD(aov(as.data.frame(as.matrix(t(otu_table(model1.ps))))$sp.14~sample_data(model1.ps)$Factor))
raw.t2<-as.data.frame(t$`sample_data(model1.ps)$Factor`[rownames(t$`sample_data(model1.ps)$Factor`) %like% "one",])

summary(aov(as.data.frame(as.matrix(t(otu_table(model1.Limma.raw))))$sp.14~sample_data(model1.Limma.raw)$Factor))
t<-TukeyHSD(aov(as.data.frame(as.matrix(t(otu_table(model1.Limma.raw))))$sp.14~sample_data(model1.Limma.raw)$Factor))
limmaVST.t2<-as.data.frame(t$`sample_data(model1.Limma.raw)$Factor`[rownames(t$`sample_data(model1.Limma.raw)$Factor`) %like% "one",])

summary(aov(as.data.frame(as.matrix(t(otu_table(model1.deseq.raw))))$sp.14~sample_data(model1.deseq.raw)$Factor))
t<-TukeyHSD(aov(as.data.frame(as.matrix(t(otu_table(model1.deseq.raw))))$sp.14~sample_data(model1.deseq.raw)$Factor))
t2<-as.data.frame(t$`sample_data(model1.deseq.raw)$Factor`[rownames(t$`sample_data(model1.deseq.raw)$Factor`) %like% "one",])
```

# environmental workflow!!

the samples so far follow the same basic distribution, with minor tweaks to the order of the taxa within the distribution. What happens if we make a distribution that has substantial patchiness among samples? This requires a very different modeling approach. 

The purpose of this next modeling framework is to illustrate how normalization affects inference of both environmental gradient detection, and interacts with more realistic uncertainty.

```{r pressure, echo=FALSE}
AllSpp<-c(paste0("spp", c(1:1724), sep="")) # make a quick list of all species functions
AllSpp<-lapply(AllSpp, get) # connect function to name
AllSpp<-unlist(AllSpp)  # format to be read by downstream functions

names(AllSpp)<-c(paste0("spp", c(1:1724)))


seeds<-round(rnorm(50,5000,100)) 
set.seed(seeds[1]) # 4945
Comm1<-base::sample(AllSpp, 200, replace=F) # pick 200 random taxa from pool
set.seed(seeds[2]) # 4902
Comm2<-base::sample(AllSpp, 200, replace=F) # pick 200 random taxa from pool
set.seed(seeds[4]) # 5098
Comm3<-base::sample(AllSpp, 200, replace=F) # pick 200 random taxa from pool

names(Comm1)
names(Comm2)
names(Comm3)




library(reshape2)
f1c1<-c(5,5,5,5,5,5) # number of selections
f1c2<-c(1,3,10,15,3,15) # mean value of selections
f1c3<-c(0.5,0.5,3,3,1,5) # SD of selections
F1.frame<-mapply(rnorm, f1c1,f1c2,f1c3) # pick Factor 1 value for each site
F1<-melt(F1.frame)

#F2
f2c1<-c(5,5,5,5,5,5) # number of selections
f2c2<-c(34,30,50,55,35,60) # mean value of selections
f2c3<-c(0.5,0.5,3,3,1,5) # SD of selections
F2.frame<-mapply(rnorm, f2c1,f2c2,f2c3) # pick Factor 2 value for each site
F2<-melt(F2.frame)

#F3
f3c1<-c(5,5,5,5,5,5) # number of selections
f3c2<-c(1,3,10,15,3,15) # mean value of selections
f3c3<-c(0.5,0.5,3,3,1,5) # SD of selections
F3.frame<-mapply(rnorm, f3c1,f3c2,f3c3) # pick Factor 3 value for each site
F3<-melt(F3.frame)

#F4
f4c1<-c(5,5,5,5,5,5) # number of selections
f4c2<-c(1,3,10,15,3,15) # mean value of selections
f4c3<-c(0.5,0.5,3,3,1,5) # SD of selections
F4.frame<-mapply(rnorm, f4c1,f4c2,f4c3) # pick Factor 4 value for each site
F4<-melt(F4.frame)

#F5
f5c1<-c(5,5,5,5,5,5) # number of selections
f5c2<-c(1,3,10,15,3,15) # mean value of selections
f5c3<-c(0.5,0.5,3,3,1,5) # SD of selections
F5.frame<-mapply(rnorm, f5c1,f5c2,f5c3) # pick Factor 5 value for each site
F5<-melt(F5.frame)

Factors<-data.frame(F1$value,F2$value,F3$value,F4$value,F5$value) # combine factors into data table
Sites<-c(paste0("Site", 1:30))
rownames(Factors)<-Sites
colnames(Factors)<-c("F1","F2","F3","F4","F5")
head(Factors)

saveRDS(Factors, "~/Documents/Github/ModelMicrobiome/Model_environment.RDS") # save environmental gradient!!

#### output response table ###

# builds an OTU table from species equations and factors
make.comm<-function(Comm1, Factors){
otu<-matrix(data=NA, nrow=nrow(Factors), ncol = length(Comm1))
Sites<-c(paste0("Site", 1:30))
for(i in 1:length(Comm1)) {
  for(row in 1:nrow(Factors)){
   otu[row,i]<-do.call(Comm1[[i]], list(Factors[row,1],Factors[row,2],Factors[row,3],Factors[row,4],Factors[row,5]))
      }
}

row.names(otu)<-Sites
colnames(otu)<-names(Comm1)
otu[otu<0]<-0
otu<-round(otu)
otu<-otu_table(otu, taxa_are_rows = FALSE)
Sa<-sample_data(Factors)
out<-phyloseq(otu, Sa)
out}

model3<-make.comm(Comm1, Factors)
t.ab<-sample_sums(model3)
sample_data(model3)$total_abund<-t.ab
model3 # needs to be subsampled yet in order to represent sequencing process

saveRDS(out, "~/Documents/GitHub/ModelMicrobiome/PS_envComm.RDS")
```

Now let's benchmark this community:
```{R}


```


the samples so far follow the same basic distribution, with minor tweaks to the order of the taxa within the distribution. What happens if we make a distribution that has substantial patchiness among samples? This requires a very different modeling approach. Here we construct a community where the samples are primarily occupied by uniique taxa. In this instance there are 10 taxa that are randomly chosen to be globally distributed. For each sampling category, there are 20 taxa that are randomly chosen to be regionally distributed. And then for each sample, there are 70 taxa that are chosen to be abundant in that sample. These taxa are chosen from a pool of YYYY taxa and are chosen with replacement between selection rounds, but without replacement within each selection round. Therefore a taxon may be chosen in the global taxon selection, the regional taxon selection and the sample-wise taxon selection. Therefore total alpha diversity among samples varies somewhat and sometimes does not equal 100. Again, between samples there may be some minor overlap between selections for sample-wise taxa; and between regions there may be some overlap between region-wise taxa. This is desireable as a representation of actual multi-scalar diversity in distribution of individual taxa. Since we have the species lists, we have a key to correct any incorrect inference about the distribution of taxa. 

It is also important to realize that the taxon abundances are modeled based on the environmental parameters. Therefore, although we have selected a taxon to exist in a site, the taxon itself may not survive in those conditions. Thus we are inflating the number of absences compared to our previous sampling approach. However, the advantage of this method over the previous one is that now we have taxa whose abundance is not solely modeled on known paramters.

```{r}
# the samples so far follow the same basic distribution, with minor tweaks to the order of the taxa within the distribution. What happens if we make a distribution that has substantial patchiness among samples?

AllSpp<-c(paste0("spp", c(1:1724), sep="")) # trying to make a quick list of all functions
AllSpp<-lapply(AllSpp, get)
AllSpp<-unlist(AllSpp)

names(AllSpp)<-c(paste0("spp", c(1:1724)))

library(plyr)
# Define list of 10 species w/ global distribution
global.spp<-names(sample(AllSpp, 10, replace=F))

# define list of species w/ regional distribution
group.spp<-NULL
group.spp$group1<-names(sample(AllSpp, 20, replace=F))
group.spp$group2<-names(sample(AllSpp, 20, replace=F))
group.spp$group3<-names(sample(AllSpp, 20, replace=F))
group.spp$group4<-names(sample(AllSpp, 20, replace=F))
group.spp$group5<-names(sample(AllSpp, 20, replace=F))
group.spp$group6<-names(sample(AllSpp, 20, replace=F))

# define list of species found at each site
rando.spp<-NULL
rando.spp$Site1<-unique(c(names(sample(AllSpp, 70, replace=F)), c(group.spp$group1), global.spp))
rando.spp$Site2<-unique(c(names(sample(AllSpp, 70, replace=F)), c(group.spp$group1), global.spp))
rando.spp$Site3<-unique(c(names(sample(AllSpp, 70, replace=F)), c(group.spp$group1), global.spp))
rando.spp$Site4<-unique(c(names(sample(AllSpp, 70, replace=F)), c(group.spp$group1), global.spp))
rando.spp$Site5<-unique(c(names(sample(AllSpp, 70, replace=F)), c(group.spp$group1), global.spp))
rando.spp$Site6<-unique(c(names(sample(AllSpp, 70, replace=F)), c(group.spp$group2), global.spp))
rando.spp$Site7<-unique(c(names(sample(AllSpp, 70, replace=F)), c(group.spp$group2), global.spp))
rando.spp$Site8<-unique(c(names(sample(AllSpp, 70, replace=F)), c(group.spp$group2), global.spp))
rando.spp$Site9<-unique(c(names(sample(AllSpp, 70, replace=F)), c(group.spp$group2), global.spp))
rando.spp$Site10<-unique(c(names(sample(AllSpp, 70, replace=F)), c(group.spp$group2), global.spp))
rando.spp$Site11<-unique(c(names(sample(AllSpp, 70, replace=F)), c(group.spp$group3), global.spp))
rando.spp$Site12<-unique(c(names(sample(AllSpp, 70, replace=F)), c(group.spp$group3), global.spp))
rando.spp$Site13<-unique(c(names(sample(AllSpp, 70, replace=F)), c(group.spp$group3), global.spp))
rando.spp$Site14<-unique(c(names(sample(AllSpp, 70, replace=F)), c(group.spp$group3), global.spp))
rando.spp$Site15<-unique(c(names(sample(AllSpp, 70, replace=F)), c(group.spp$group3), global.spp))
rando.spp$Site16<-unique(c(names(sample(AllSpp, 70, replace=F)), c(group.spp$group4), global.spp))
rando.spp$Site17<-unique(c(names(sample(AllSpp, 70, replace=F)), c(group.spp$group4), global.spp))
rando.spp$Site18<-unique(c(names(sample(AllSpp, 70, replace=F)), c(group.spp$group4), global.spp))
rando.spp$Site19<-unique(c(names(sample(AllSpp, 70, replace=F)), c(group.spp$group4), global.spp))
rando.spp$Site20<-unique(c(names(sample(AllSpp, 70, replace=F)), c(group.spp$group4), global.spp))
rando.spp$Site21<-unique(c(names(sample(AllSpp, 70, replace=F)), c(group.spp$group5), global.spp))
rando.spp$Site22<-unique(c(names(sample(AllSpp, 70, replace=F)), c(group.spp$group5), global.spp))
rando.spp$Site23<-unique(c(names(sample(AllSpp, 70, replace=F)), c(group.spp$group5), global.spp))
rando.spp$Site24<-unique(c(names(sample(AllSpp, 70, replace=F)), c(group.spp$group5), global.spp))
rando.spp$Site25<-unique(c(names(sample(AllSpp, 70, replace=F)), c(group.spp$group5), global.spp))
rando.spp$Site26<-unique(c(names(sample(AllSpp, 70, replace=F)), c(group.spp$group6), global.spp))
rando.spp$Site27<-unique(c(names(sample(AllSpp, 70, replace=F)), c(group.spp$group6), global.spp))
rando.spp$Site28<-unique(c(names(sample(AllSpp, 70, replace=F)), c(group.spp$group6), global.spp))
rando.spp$Site29<-unique(c(names(sample(AllSpp, 70, replace=F)), c(group.spp$group6), global.spp))
rando.spp$Site30<-unique(c(names(sample(AllSpp, 70, replace=F)), c(group.spp$group6), global.spp))

rando.spp # examine lists

# format lists to be read by downstream functions
final.list<-for(i in rando.spp){
  l<-lapply(names(rando.spp[i]), get)
  names(l)<-names(i)
  l
  }

# function for making the environmental community !!
make.comm2<-function(rando.spp, Factors){
l1<-NULL
for (i in 1:length(rando.spp[[1]])){
  l1[i]<-do.call(rando.spp[[1]][i], list(Factors[1,1],Factors[1,2],Factors[1,3],Factors[1,4],Factors[1,5]))
  }
#l1<-data.frame("Site1"=l1, "Spp"=rando.spp[[1]])
names(l1)<-rando.spp[[1]]
for (r in 2:nrow(Factors)) # for each site...
{ l2<-NULL
  for (i in 1:length(rando.spp[[r]])){  # for each species in site...
    l2[i]<-do.call(rando.spp[[r]][i], list(Factors[r,1],Factors[r,2],Factors[r,3],Factors[r,4],Factors[r,5]))
    }
  names(l2)<-rando.spp[[r]]
  l1<-merge(as.data.frame(l1),as.data.frame(l2), by=0, all=T)
  rownames(l1)<-l1$Row.names
  colnames(l1)[colnames(l1) == "l1"] <- "Site1"
 colnames(l1)[colnames(l1) == "l2"] <- paste("Site", r, sep="")
 l1<-l1[,-1]
  }
l1<-round(l1)
l1[is.na(l1)]<-0
l1[l1<0]<-0
otu<-otu_table(l1, taxa_are_rows = T)
Sa<-sample_data(Factors)
out<-phyloseq(otu, Sa)
out
}

model4<-make.comm2(rando.spp, Factors)
model4 # still needs to be subsampled to replicate sequencing.

```


Now let's benchmark this community:
```{R}


```



What is the effect of increasing the variation of sequencing depth? 
```{R}


```



Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.


Build a script to run the following platforms:

DESeq2
Limma Trend
EdgeR
BBSeq 
DSS
BaySeq
ShrinkBayes
PoissonSeq
