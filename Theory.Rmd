---
title: "Standardized Approaches to Microbial Metabarcoding Studies"
author: "Dietrich Epp Schmidt"
date: "7/25/2019"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

## introduction

For some time there has been contentious debate over the appropriate data handling techniques for microbial sequence data. The center of the controversy is the question of whether we should or should not rarefy our samples. In this case, rarefaction means subsampling each sample to a certain threshold. Typically, the practitioner will determine an arbitrary minimum count and rarefy all samples down to this count. This technique is employed to adress the lack of uniform sampling effort that is inherent in sequence data (CITATION). It should be pointed out here that subsampling sequences to an equal number of sequences is not in fact normalizing sampling effort. McMurdie and Holmes argued in 2014 that no sequences should be omitted for the purpose of normalizing sampling effort. Their argument essentially is that to do causes us to lose the power to detect taxa that respond, and increased in false positives. Later, Weiss et al countered that their method unfairly handicapped rarefied datasets in their modeling. The center of the dispute was in how the mock community was constructed, and scored. McCmurdie and Holmes had manually increased the abundance of certain taxa in the dataset. They only counted those that they had manipulated as being true positives. But if one taxon should increase in abundance, then as a proportion all other taxa will decrease. These are true effects in the sense that the relative abundance of each taxon is actually changing within the dataset. McMurdie and Holmes, however, scored those detections as false positives and found that rarefied data had a much higher probability of detecting such false positives. Weiss et all argued that in fact these were true positives, suggesting that rarefaction actually increased the sensitivity of the differential abundance analysis. They proposed using a mock community where manipulated taxa are reciprocally balanced such that they do not affect the relative abundance of any other taxon (if one taxon is increased by 10, another is decreased by 10 for balance). This differential abundance test showing that rarefaction did sometimes improve true positive detection, depending on the context. It should be pointed out here that strictly speaking, this type of behavior (i.e. perfectly balanced differences in taxon abundance) does not occure in nature.

What is perhaps most curious about this debate is what is not addressed: the underlying reason that ecologists seek to rarefy data. The concept of effort in sampling is quite literally a representation of physical effort in much of macro-ecology. It seeks to ensure that, for example, the dimensions of area surveyed in each sampling location of a study are equal so that the results are directly comparable. In sequencing studies, effort cannot be controled a priori either physically or procedurally by the scientist because the sequencing machine acts much like a random number generator in determining how many counts (often interpreted as effort) is devoted to each sample. But counts themselves are not intrinsically a measure of effort. If population A has twice the density of organisms than population B, then if samples of both are represented by the same number of sequences, then twice the effort has been expended on sampling population B compared to population A. Effort per unit sampled is important to normalize because it provides a standardization that allows the scientist to infer the density of the organisms. Therefore, to control effort is to normalize for organismal density in the real world. This key observation is lost in the discussion as none of the parties (and none of the softwares employed) include a standard protocol for normalizing for population density across samples. This is because the major publicly available softwares in this realm (particularly DESeq2, EdgeR, and Limma) were developed to determine differential expression of genes within organisms - not the differential abundance of organisms in the environment. They were designed to work with either with RNA-seq data, and often are expansions of software originally developed for micro-array analysis. In this data context, the overall abundance of RNA in the cell is rarely, if ever, an important factor to consider. Since gene expression is highly interdependent within an organism, expression data is best understood as a distribution where autocorrelation of the sequence counts is assumed. Therefore, all of these algorithms include normalization protocol for sample-wise sequencing depth within the GLM. However, for microbiome work the density of the overall community is of paramount importance. The diversity of metabolic strategies is such that groups of microbes may be essentially independent in their growth, each either relying on or being limited by different substrates. If one group becomes more abundant while the other remains at the same abundance level, it is impossible to parse whether one grew or the other died off simply using distributional (or proportional) data. In order to accurately infer the environmental drivers of each taxon, the practicioner needs to model a population density correction into the GLM. Thus, to understand how the environment shapes any microbial community, there must be an independent measure of overall community abundance. The power of external normalization has recently been demonstrated as a XYZ et. al published a paper where they were able to achieve nearly quantitative estimation of taxon abundance using QPCR as an external measure of total bacterial density. 

From the ecological perspective, much of the modeling for expression data has been done backwards. Typically, the modeler generates a random base dataset, manipulate part of it to generate a "treatment effect," then tests if the algorithm accurately identifies the manipulation. This does not allow the modeler to test the algorithm's ability to infer the state of an original population; in essence this is just a test of the algorithm's ability to infer the state of the sample. In order to benchmark the algorithm's ability to infer the population parameters, we must make a model population, sample from the model population, and then supply the algorithm with the sample data and determine its accuracy in predicting the actual state of the population. None of the popular algorithms employed for differential abundance testing have been benchmarked in this way. It also has the advantage that it eliminates subjectivity in interpreting the results because there is a modeled population in the simulation. There can be no quibble over the proper scoring for the benchmark.

Fragments:
XYZ et al's data demonstrates that highly abundant taxa are easy to model using an external... This is because normalizing the distribution to the density will
Rarefaction is often misapplied in The advantage of rarefaction is that it allows us to normalize the sequence distributio

##Methods and Results

First we use a simplistic community model framework designed to elucidate explicitly the theoretical weaknesses of different preprossessing and statistical approaches. Then we present a flexible modeling platform that allows us to construct dynamic and nuanced community models to test the bounds of the inferential statistical software.

![Analysis Workflow](images/week3/silly-dog.png)
# Model Community 1
Construct a model population. In this case we used the fibonacci sequence as a base for taxon abundance curves. We create several different detection scenarios for how the taxa could respond to the environment. Here the only environmental data is total population for each sample.

```{r}
library(phyloseq)

fib1<-c(1,1,2,3,5,8,13,21,34,55,89,144,233,377,610) #1596 (Reference conditions)
fib2<-fib1*2 #3192 (wrt ref: all taxa increase) ;(expected false: all the same)
fib3<-c(1,1,2,3,8,5,13,21,34,55,89,144,377,233,610) # (wrt ref: 5,13 increases, 6,14 decreases); expected false: NA
fib4<-fib3*2 # (wrt ref: all taxa increase); expected false(5,13 increases, 6,14 decreases)
fib5<-c(1,1,2,3,8,8,13,21,34,55,89,233,233,377,610) # a (wrt ref: 5,12 increases) ;expected false(NA)
fib6<-fib5*2 # (wrt ref: all taxa increase); expected false(5,12 increases)
fib7<-c(1,1,2,3,5,8,8,21,34,55,89,144,233,377,377) # (wrt ref: 7,15 decreases); expected false(NA)
fib8<-fib7*2 # (wrt ref: all taxa increase) ; expected false(7,15 decreases)
fib9<-c(1,1,2,0,5,0,13,21,34,55,89,144,0,377,610) # identify zeros; expected false(NA)
fib10<-fib9*2 # (wrt ref: 4,6,13 decrease, all others increase); expected false(4,6,13 decrease)
fib11<-c(1,1,2,3,5,8,13,21,34,55,89,144,466,754,1220) # (wrt ref: only top 3 increase) ; expected false(bottom 12 decrease)
fib12<-c(2,2,4,6,10,16,26,42,68,11,178,288,233,377,610) # (wrt ref: all increase except top 3 stay same); expected false(top 3 decrease)

#now let's make a community where the majority of taxa are only in one site
#first 15 are ubiquitous
fib13<-c(5,10,100,5,10,20,1,1,1,280,124,100,790,540,420,233,377,610,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,0)
fib14<-c(5,10,100,0,0,0,540,420,1,1,1,280,5,10,20,0,0,0,0,233,377,610,0,0,0,0,0,0,0,0,0,0,0,0,0)
fib15<-c(5,10,100,0,0,0,540,420,1,1,1,280,5,540,420,0,0,0,0,0,0,0,0,0,233,377,610,0,0,0,0,0,0,0,0)
fib16<-c(5,10,100,0,0,0,540,420,1,1,1,280,5,540,420,0,0,0,0,0,0,0,0,0,0,0,0,233,377,610,0,0,0,0,0)
fib17<-c(5,rep(1,5),0,0,0,0,0,0,0,0,0,0,0,0,0,0,0,89,144,466,233,377,610,0,0,0,0,500,50,100,200)

comm<-data.frame(fib1,fib2,fib3,fib4,fib5,fib6,fib7,fib8,fib9,fib10,fib11,fib12)
rownames(comm)<-paste(rep("sp",15), c(1:15), sep=".")

comm2<-data.frame(fib13, fib14, fib15, fib16, fib17)
rownames(comm2)<-paste(rep("sp",35), c(1:35), sep=".")
```

Now let's sample from our defined populations, and construct our sampled phyloseq object. This function is meant to simulate the sequencing machine. For this, we employ a function that samples from our population with replacement. Replacement is important because in almost all metabarcoding protocol there is amplification that creates artificial copies that follow the distribution, but not total number. In effect, sequencing is sampling with replacement. This function models sequencing also by assigning random total sample values and (unless otherwise specified) generates a random total number of sequences per sample with a mean value of 250 sequences and a variance of 75 sequences.

```{r}

make.table<-function(comm, r){ 
  
  m<-as.data.frame(t(table(sample(rownames(comm),rnorm(1, 250, 75), replace=T, prob=comm[,1]/sum(comm[,1])))))
   m<-m[,colnames(m)!="Var1"]
   colnames(m)[colnames(m)=="Freq"]<-paste("site", 1, r, sep=".")
    m2<-as.data.frame(t(table(sample(rownames(comm),rnorm(1, 250, 75), replace=T, prob=comm[,2]/sum(comm[,2])))))
   m<-merge(m, m2, by="Var2", all=T)
    m<-m[,colnames(m)!="Var1"]
    colnames(m)[colnames(m)=="Freq"]<-paste("site", 2, r, sep=".")
    for(i in 3:ncol(comm)){
     a<-as.data.frame(t(table(sample(rownames(comm),rnorm(1, 250, 75), replace=T, prob=comm[,i]/sum(comm[,i])))))
     a<-a[,colnames(a)!="Var1"]
     m<-merge(m,a, by="Var2", all=T)
      m<-m[,colnames(m)!="Var1"]
      colnames(m)[colnames(m)=="Freq"]<-paste("site", i, r, sep=".")
      
    }
    #colnames(m)<-c("Var1", paste0("site",c(1:12), r, sep="."))
      #names(m)[names(m)=="Var2"]<-paste0("Sample", r,)
      rownames(m)<-m$Var2
      m
    }

model.rarefy<-function(comm, rep){
 a<-make.table(comm, r=1)
 #a<-a[,colnames(a)!="Var1"]
 for(r in 2:rep){
   b<-make.table(comm, r)
   a<-merge(a,b,by="Var2", all=T)
   a<-a[,colnames(a)!="Var1"]}
   rownames(a)<-a$Var2
   a<-a[,colnames(a)!="Var2"]
   a[is.na(a)] <- 0
   a
 }




model1.otu<-model.rarefy(comm, 3) # works
s.order1<-c("site.1.1", "site.1.2", "site.1.3", "site.2.1", "site.2.2", "site.2.3", "site.3.1", "site.3.2", "site.3.3", "site.4.1", "site.4.2", "site.4.3", "site.5.1","site.5.2","site.5.3", "site.6.1","site.6.2","site.6.3", "site.7.1","site.7.2","site.7.3", "site.8.1","site.8.2","site.8.3", "site.9.1","site.9.2","site.9.3", "site.10.1","site.10.2","site.10.3", "site.11.1","site.11.2","site.11.3", "site.12.1","site.12.2","site.12.3")
model1.otu<-model1.otu[,s.order1]

model2.otu<-model.rarefy(comm=comm2, 5) # works
s.order2<-c("site.1.1", "site.1.2", "site.1.3", "site.1.4", "site.1.5", "site.2.1", "site.2.2", "site.2.3", "site.2.4","site.2.5","site.3.1", "site.3.2", "site.3.3","site.3.4","site.3.5", "site.4.1", "site.4.2", "site.4.3","site.4.4","site.4.5", "site.5.1","site.5.2","site.5.3", "site.5.4","site.5.5")
model2.otu<-model2.otu[,s.order2]

# construct the metadata
meta1<-data.frame("Factor" = as.factor(c(rep("one",3), rep("two",3),rep("three",3),rep("four",3), rep("five",3), rep("six",3),rep("seven",3),rep("eight",3),rep("nine",3),rep("ten",3),rep("eleven",3),rep("twelve",3))), "Sample" = c("site.1.1", "site.1.2", "site.1.3", "site.2.1", "site.2.2", "site.2.3", "site.3.1", "site.3.2", "site.3.3", "site.4.1", "site.4.2", "site.4.3", "site.5.1","site.5.2","site.5.3", "site.6.1","site.6.2","site.6.3", "site.7.1","site.7.2","site.7.3", "site.8.1","site.8.2","site.8.3", "site.9.1","site.9.2","site.9.3", "site.10.1","site.10.2","site.10.3", "site.11.1","site.11.2","site.11.3", "site.12.1","site.12.2","site.12.3"), "Density"=c(rep(sum(fib1), 3),rep(sum(fib2), 3),rep(sum(fib3), 3),rep(sum(fib4), 3),rep(sum(fib5), 3),rep(sum(fib6), 3),rep(sum(fib7), 3),rep(sum(fib8), 3),rep(sum(fib9), 3),rep(sum(fib10), 3),rep(sum(fib11), 3),rep(sum(fib12), 3)))
rownames(meta1)<-meta$Sample
model1.ps<-phyloseq(otu_table(model1.otu, taxa_are_rows = T), sample_data(meta1))
sample_data(model1.ps)$Total_abundance<-sample_sums(model1.ps)
sample_data(model1.ps)$DensityF.model1<-sample_data(model1.ps)$Density/mean(sample_data(model1.ps)$Density)


meta2<-data.frame("Factor" = as.factor(c(rep("one",5), rep("two",5),rep("three",5),rep("four",5), rep("five",5))), "Sample" = c("site.1.1", "site.1.2", "site.1.3", "site.1.4", "site.1.5", "site.2.1", "site.2.2", "site.2.3", "site.2.4","site.2.5","site.3.1", "site.3.2", "site.3.3","site.3.4","site.3.5", "site.4.1", "site.4.2", "site.4.3","site.4.4","site.4.5", "site.5.1","site.5.2","site.5.3", "site.5.4","site.5.5"), "Density"=c(rep(sum(fib13), 5),rep(sum(fib14), 5),rep(sum(fib15), 5),rep(sum(fib16), 5),rep(sum(fib17), 5)))
rownames(meta2)<-meta2$Sample
model2.ps<-phyloseq(otu_table(model2.otu, taxa_are_rows = T), sample_data(meta2))
sample_data(model2.ps)$Total_abundance<-sample_sums(model2.ps)
sample_data(model2.ps)$DensityF.model2<-sample_data(model2.ps)$Density/mean(sample_data(model2.ps)$Density)


saveRDS(model.ps, "~/Documents/GitHub/ModelMicrobiome/ModelPS.RDS")
?phyloseq::subset_samples
```
There are several philosophies about how to normalize the sample data. Some say not to do any normalization, to leave counts as they are (method=untouched); some say that the sequence counts should be variance stabilized- there are several different methods (method=deseqVST and limmaVST); some say that the data should be rarefied to an even sampling depth (method=evenRarefied); some say that we should rarefy to depths that are proportional to an independent measure of overall population size (method=proportionalRarefied); and finally some say that we should simply scale the sample distributions to an independent measure of population size (method=scaled). We do all of these for comparison. We will simulate QPCR of our target gene using the sums of the modeled population. This will be a vector called (Density)

```{r}

make.rarefy<-function(x, level){
  require(phyloseq)
  require(vegan)
  
  if (length(level)==1){
     p<-prune_samples(sample_sums(x)>=level, x)
  
  if (nsamples(x)>nsamples(p)){warning(as.character(nsamples(x)-nsamples(p)), " samples have been removed because they are lower than rarefaction limit")}
     
  r<-as.data.frame(as.matrix(otu_table(p)))
  meta<-sample_data(p)
  rr<-rrarefy(t(r), level) #use lowest value
  ps<-phyloseq(otu_table(t(rr), taxa_are_rows = T), sample_data(meta))
  ps} else {
    sample_data(x)$adj<-level
    
     p<-prune_samples(sample_sums(x)>=level, x)
  if (nsamples(x)>nsamples(p)){warning(as.character(nsamples(x)-nsamples(p)), " samples have been removed because they are lower than rarefaction limit")}
    
    r<-as.data.frame(as.matrix(otu_table(p)))
  meta<-sample_data(p)
  rr<-rrarefy(t(r), meta$adj)
  ps<-phyloseq(otu_table(t(rr), taxa_are_rows = T), sample_data(meta))
  ps
  }
}

make.scaled<-function(ps, val, scale){
  scaled<-data.frame(mapply(`*`, data.frame(as.matrix(otu_table(transform_sample_counts(ps, function(x) x/sum(x))))), scale * val))# sample_data(ps)$val))
  names<-rownames(data.frame(as.matrix(otu_table(ps))))
  rownames(scaled)<-names
  scaled<-round(scaled)

  p2<-ps
  otu_table(p2)<- otu_table(scaled, taxa_are_rows=T)
  p2
}

# untouched ####
untouched<-model.ps

# deseqVST ####
gm_mean = function(x, na.rm=TRUE){
  exp(sum(log(x[x > 0]), na.rm=na.rm) / length(x))
} # relative abundance

make.deseqVST<-function(ps, Factor, l){
r<-phyloseq_to_deseq2(ps, ~Factor)
geoMeans = apply(counts(r), 1, gm_mean)
dds = estimateSizeFactors(r, geoMeans = geoMeans)
#dds<-DESeqDataSetFromMatrix(r, sample_data(ps), design=~Factor)
#dds = estimateSizeFactors(dds)
if (l==1){dds = estimateDispersions(dds)} else {
dds <- estimateDispersionsGeneEst(dds)
 dispersions(dds) <- mcols(dds)$dispGeneEst
  }
vst = getVarianceStabilizedData(dds)
deseqVST<-ps
otu_table(deseqVST) <- otu_table(vst, taxa_are_rows = TRUE)
deseqVST
}
# LimmaVST ###

make.limmaVST<-function(ps, Factor){
  
  counts<-as.data.frame(as.matrix(otu_table(ps)))
  factors<-unlist(sample_data(ps)[,Factor])
  design<-model.matrix(~factors)
  dge <- DGEList(counts=counts)
  dge <- calcNormFactors(dge) #what happens if we don't do this step?
  v<-voom(dge, design, plot=F)
  LimmaVST<-ps
  otu_table(LimmaVST)<-otu_table(v$E, taxa_are_rows = T)
  LimmaVST
}


```

Let's now run the functions to get all the different combinations of preprocessing...


```{r}

# evenRarefied ####
model1.eRare<-make.rarefy(model1.ps, 100)  
model2.eRare<-make.rarefy(model2.ps, 100)


# proportionalRarefied ####
model1.pRare<-make.rarefy(model1.ps, 100 * sample_data(model1.ps)$DensityF.model1)  
model2.pRare<-make.rarefy(model2.ps, 100 * sample_data(model2.ps)$DensityF.model2)  


# scaled ####
model1.scaled<-make.scaled(model1.ps, val=sample_data(model1.ps)$DensityF.model1, scale = 100)
model2.scaled<-make.scaled(model2.ps, val=sample_data(model2.ps)$DensityF.model2, scale = 100)

# limma vst ####
model1.Limma.raw<-make.limmaVST(model1.ps, "Factor")
model1.Limma.eRare<-make.limmaVST(model1.eRare, "Factor")
model1.Limma.pRare<-make.limmaVST(model1.pRare, "Factor")
model1.Limma.scaled<-make.limmaVST(model1.scaled, "Factor")

model2.Limma.raw<-make.limmaVST(model2.ps, "Factor")
model2.Limma.eRare<-make.limmaVST(model2.eRare, "Factor")
model2.Limma.pRare<-make.limmaVST(model2.pRare, "Factor")
model2.Limma.scaled<-make.limmaVST(model2.scaled, "Factor")

# Deseq2 vst ####

model1.deseq.raw<-make.deseqVST(model1.ps, "Factor", l=1)
model1.deseq.eRare<-make.deseqVST(model1.eRare, "Factor", l=2) # error not enough dispersion
model1.deseq.pRare<-make.deseqVST(model1.pRare, "Factor") # error not enough dispersion
model1.deseq.scaled<-make.deseqVST(model1.scaled, "Factor") # error not enough dispersion

model2.deseq.raw<-make.deseqVST(model2.ps, "Factor")
model2.deseq.eRare<-make.deseqVST(model2.eRare, "Factor")
model2.deseq.pRare<-make.deseqVST(model2.pRare, "Factor")
model2.deseq.scaled<-make.deseqVST(model2.scaled, "Factor")

```
Build a script to run the following platforms:

DESeq2
Limma Trend <- uses EdgeR 
EdgeR <- already there
BBSeq  <- nope! bisulfide sequencing...
DSS <- um...
BaySeq <- meh
ShrinkBayes <- meh
PoissonSeq <- this one!!

Now we build out the test key
```{r}
# zero is no change, 1 is increase, -1 is decrease
model.key<-data.frame("one"=c(rep(0,15)),"two"=c(rep(1,15)), "three" = c(0,0,0,0,1,-1,0,0,0,0,0,0,1,-1,0), "four" =c(rep(1, 15)), "five"= c(0,0,0,0,1,0,0,0,0,0,0,1,0,0,0), "six"=c(rep(1, 15)), "seven"=c(0,0,0,0,0,0,-1,0,0,0,0,0,0,0,-1), "eight"=c(rep(1,15)), "nine"=c(0,0,0,-1,0,-1,0,0,0,0,0,0,-1,0,0), "ten"=c(1,1,1,-1,1,-1,1,1,1,1,1,1,-1,1,1), "eleven"=c(0,0,0,0,0,0,0,0,0,0,0,0,1,1,1), "twelve"= c(1,1,1,1,1,1,1,1,1,1,1,1,0,0,0))
rownames(model.key)<-paste("sp", 1:15, sep=".")
model.key
```

Run test of environmental correlation:
```{r}
<-function(x, model){
  require(vegan)
  a<-adonis()
  summary(a)
  
}
```
If we want to test for all decreasing effects, we can set number two as zero in the comparison tables.


Run test of indicators:
```{r}

library(data.table)
model1.Raw.Limmaindics<-as.data.frame(limma.Indics(model1.ps, "Factor"))
model1.eRare.Limmaindics<-as.data.frame(limma.Indics(model1.eRare, "Factor"))
model1.pRare.Limmaindics<-as.data.frame(limma.Indics(model1.pRare, "Factor"))
model1.scaled.Limmaindics<-as.data.frame(limma.Indics(model1.scaled, "Factor"))

summary(aov(as.data.frame(as.matrix(t(otu_table(model1.eRare))))$sp.14~sample_data(model1.eRare)$Factor))
t<-TukeyHSD(aov(as.data.frame(as.matrix(t(otu_table(model1.eRare))))$sp.14~sample_data(model1.eRare)$Factor))
t2<-as.data.frame(t$`sample_data(model1.eRare)$Factor`[rownames(t$`sample_data(model1.eRare)$Factor`) %like% "one",])

summary(aov(as.data.frame(as.matrix(t(otu_table(model1.pRare))))$sp.14~sample_data(model1.pRare)$Factor))
t<-TukeyHSD(aov(as.data.frame(as.matrix(t(otu_table(model1.pRare))))$sp.14~sample_data(model1.pRare)$Factor))
prare.t2<-as.data.frame(t$`sample_data(model1.pRare)$Factor`[rownames(t$`sample_data(model1.pRare)$Factor`) %like% "one",])

summary(aov(as.data.frame(as.matrix(t(otu_table(model1.scaled))))$sp.14~sample_data(model1.scaled)$Factor))
t<-TukeyHSD(aov(as.data.frame(as.matrix(t(otu_table(model1.scaled))))$sp.14~sample_data(model1.scaled)$Factor))
scaled.t2<-as.data.frame(t$`sample_data(model1.scaled)$Factor`[rownames(t$`sample_data(model1.scaled)$Factor`) %like% "one",])

summary(aov(as.data.frame(as.matrix(t(otu_table(model1.ps))))$sp.14~sample_data(model1.ps)$Factor))
t<-TukeyHSD(aov(as.data.frame(as.matrix(t(otu_table(model1.ps))))$sp.14~sample_data(model1.ps)$Factor))
raw.t2<-as.data.frame(t$`sample_data(model1.ps)$Factor`[rownames(t$`sample_data(model1.ps)$Factor`) %like% "one",])

summary(aov(as.data.frame(as.matrix(t(otu_table(model1.Limma.raw))))$sp.14~sample_data(model1.Limma.raw)$Factor))
t<-TukeyHSD(aov(as.data.frame(as.matrix(t(otu_table(model1.Limma.raw))))$sp.14~sample_data(model1.Limma.raw)$Factor))
limmaVST.t2<-as.data.frame(t$`sample_data(model1.Limma.raw)$Factor`[rownames(t$`sample_data(model1.Limma.raw)$Factor`) %like% "one",])

summary(aov(as.data.frame(as.matrix(t(otu_table(model1.deseq.raw))))$sp.14~sample_data(model1.deseq.raw)$Factor))
t<-TukeyHSD(aov(as.data.frame(as.matrix(t(otu_table(model1.deseq.raw))))$sp.14~sample_data(model1.deseq.raw)$Factor))
t2<-as.data.frame(t$`sample_data(model1.deseq.raw)$Factor`[rownames(t$`sample_data(model1.deseq.raw)$Factor`) %like% "one",])


heatdf<-data.frame("AOV.scaled"=scaled.t2$`p adj`, "AOV.Prare"=prare.t2$`p adj`, "AOV.raw"=raw.t2$`p adj`, "AOV.limmaVST"=limmaVST.t2$`p adj`)
k1<-c(1,1,0,1,0,0,1,1,1,0,1)
k1<-c(T,T,F,T,F,F,T,T,T,F,T)
k.df<-c(k1, k1, k1, k1)
heatdf<-heatdf<=0.05

heat.df<-heatdf==k.df
heat.df$ID<-rownames(t2)

heat<-melt(heat.df, id.var=ID)
ggplot(heat, aes())


rownames(heat.df)<-rownames(t2)
heat.df<-lapply(heat.df, numeric)
heatmap(numeric(as.character(heat.df)))
```

```{R}

model2.Raw.Limmaindics<-as.data.frame(limma.Indics2(model2.ps, "Factor"))
model2.eRare.Limmaindics<-as.data.frame(limma.Indics(model2.eRare, "Factor"))
model2.pRare.Limmaindics<-as.data.frame(limma.Indics(model2.pRare, "Factor"))
model2.scaled.Limmaindics<-as.data.frame(limma.Indics(model2.scaled, "Factor"))

summary(aov(as.data.frame(as.matrix(t(otu_table(model1.eRare))))$sp.14~sample_data(model1.eRare)$Factor))
t<-TukeyHSD(aov(as.data.frame(as.matrix(t(otu_table(model1.eRare))))$sp.14~sample_data(model1.eRare)$Factor))
t2<-as.data.frame(t$`sample_data(model1.eRare)$Factor`[rownames(t$`sample_data(model1.eRare)$Factor`) %like% "one",])

summary(aov(as.data.frame(as.matrix(t(otu_table(model1.pRare))))$sp.14~sample_data(model1.pRare)$Factor))
t<-TukeyHSD(aov(as.data.frame(as.matrix(t(otu_table(model1.pRare))))$sp.14~sample_data(model1.pRare)$Factor))
prare.t2<-as.data.frame(t$`sample_data(model1.pRare)$Factor`[rownames(t$`sample_data(model1.pRare)$Factor`) %like% "one",])

summary(aov(as.data.frame(as.matrix(t(otu_table(model1.scaled))))$sp.14~sample_data(model1.scaled)$Factor))
t<-TukeyHSD(aov(as.data.frame(as.matrix(t(otu_table(model1.scaled))))$sp.14~sample_data(model1.scaled)$Factor))
scaled.t2<-as.data.frame(t$`sample_data(model1.scaled)$Factor`[rownames(t$`sample_data(model1.scaled)$Factor`) %like% "one",])

summary(aov(as.data.frame(as.matrix(t(otu_table(model1.ps))))$sp.14~sample_data(model1.ps)$Factor))
t<-TukeyHSD(aov(as.data.frame(as.matrix(t(otu_table(model1.ps))))$sp.14~sample_data(model1.ps)$Factor))
raw.t2<-as.data.frame(t$`sample_data(model1.ps)$Factor`[rownames(t$`sample_data(model1.ps)$Factor`) %like% "one",])

summary(aov(as.data.frame(as.matrix(t(otu_table(model1.Limma.raw))))$sp.14~sample_data(model1.Limma.raw)$Factor))
t<-TukeyHSD(aov(as.data.frame(as.matrix(t(otu_table(model1.Limma.raw))))$sp.14~sample_data(model1.Limma.raw)$Factor))
limmaVST.t2<-as.data.frame(t$`sample_data(model1.Limma.raw)$Factor`[rownames(t$`sample_data(model1.Limma.raw)$Factor`) %like% "one",])

summary(aov(as.data.frame(as.matrix(t(otu_table(model1.deseq.raw))))$sp.14~sample_data(model1.deseq.raw)$Factor))
t<-TukeyHSD(aov(as.data.frame(as.matrix(t(otu_table(model1.deseq.raw))))$sp.14~sample_data(model1.deseq.raw)$Factor))
t2<-as.data.frame(t$`sample_data(model1.deseq.raw)$Factor`[rownames(t$`sample_data(model1.deseq.raw)$Factor`) %like% "one",])
```

# environmental workflow!!

the samples so far follow the same basic distribution, with minor tweaks to the order of the taxa within the distribution. What happens if we make a distribution that has substantial patchiness among samples? This requires a very different modeling approach. 

The purpose of this next modeling framework is to illustrate how normalization affects inference of both environmental gradient detection, and interacts with more realistic uncertainty.

```{r pressure, echo=FALSE}
AllSpp<-c(paste0("spp", c(1:1724), sep="")) # trying to make a quick list of all functions
AllSpp<-lapply(AllSpp, get)
AllSpp<-unlist(AllSpp)

names(AllSpp)<-c(paste0("spp", c(1:1724)))


seeds<-round(rnorm(50,5000,100)) 
set.seed(seeds[1]) # 4945
Comm1<-base::sample(AllSpp, 200, replace=F)
set.seed(seeds[2]) # 4902
Comm2<-base::sample(AllSpp, 200, replace=F)
set.seed(seeds[4]) # 5098
Comm3<-base::sample(AllSpp, 200, replace=F)

names(Comm1)
names(Comm2)
names(Comm3)




library(reshape2)
f1c1<-c(5,5,5,5,5,5)
f1c2<-c(1,3,10,15,3,15)
f1c3<-c(0.5,0.5,3,3,1,5)
F1.frame<-mapply(rnorm, f1c1,f1c2,f1c3)
F1<-melt(F1.frame)

#F2
f2c1<-c(5,5,5,5,5,5)
f2c2<-c(34,30,50,55,35,60)
f2c3<-c(0.5,0.5,3,3,1,5)
F2.frame<-mapply(rnorm, f2c1,f2c2,f2c3)
F2<-melt(F2.frame)

#F3
f3c1<-c(5,5,5,5,5,5)
f3c2<-c(1,3,10,15,3,15)
f3c3<-c(0.5,0.5,3,3,1,5)
F3.frame<-mapply(rnorm, f3c1,f3c2,f3c3)
F3<-melt(F3.frame)

#F4
f4c1<-c(5,5,5,5,5,5)
f4c2<-c(1,3,10,15,3,15)
f4c3<-c(0.5,0.5,3,3,1,5)
F4.frame<-mapply(rnorm, f4c1,f4c2,f4c3)
F4<-melt(F4.frame)

#F5
f5c1<-c(5,5,5,5,5,5)
f5c2<-c(1,3,10,15,3,15)
f5c3<-c(0.5,0.5,3,3,1,5)
F5.frame<-mapply(rnorm, f5c1,f5c2,f5c3)
F5<-melt(F5.frame)

Factors<-data.frame(F1$value,F2$value,F3$value,F4$value,F5$value) # environment
Sites<-c(paste0("Site", 1:30))
rownames(Factors)<-Sites
colnames(Factors)<-c("F1","F2","F3","F4","F5")
head(Factors)

saveRDS(Factors, "~/Documents/Github/ModelMicrobiome/Model_environment.RDS") # save environmental gradient!!

#### output response table ###

otu<-make.comm(Comm1, Factors)
row.names(otu)<-Sites
colnames(otu)<-names(Comm1)
otu[otu<0]<-0
otu<-round(otu)
otu<-otu_table(otu, taxa_are_rows = FALSE)
Sa<-sample_data(Factors)
out<-phyloseq(otu, Sa)
t.ab<-sample_sums(out)
sample_data(out)$total_abund<-t.ab
out

saveRDS(out, "~/Documents/GitHub/ModelMicrobiome/PS_envComm.RDS")
```

the samples so far follow the same basic distribution, with minor tweaks to the order of the taxa within the distribution. What happens if we make a distribution that has substantial patchiness among samples? This requires a very different modeling approach.

```{r}
# the samples so far follow the same basic distribution, with minor tweaks to the order of the taxa within the distribution. What happens if we make a distribution that has substantial patchiness among samples?


spp.list1<-base::sample(AllSpp, 40, replace=F)
spp.list2<-setdiff(names(AllSpp), names(spp.list1))
spp.list2<-lapply(spp.list2, get)
spp.list2<-unlist(spp.list2)

length(spp.list2)
```
Note that the `echo = FALSE` parameter was added to the code chunk to prevent printing of the R code that generated the plot.


Build a script to run the following platforms:

DESeq2
Limma Trend
EdgeR
BBSeq 
DSS
BaySeq
ShrinkBayes
PoissonSeq
